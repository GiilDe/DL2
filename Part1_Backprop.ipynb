{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {}
      },
      "source": [
        "$$\n",
        "\\newcommand{\\mat}[1]{\\boldsymbol {#1}}\n",
        "\\newcommand{\\mattr}[1]{\\boldsymbol {#1}^\\top}\n",
        "\\newcommand{\\matinv}[1]{\\boldsymbol {#1}^{-1}}\n",
        "\\newcommand{\\vec}[1]{\\boldsymbol {#1}}\n",
        "\\newcommand{\\vectr}[1]{\\boldsymbol {#1}^\\top}\n",
        "\\newcommand{\\rvar}[1]{\\mathrm {#1}}\n",
        "\\newcommand{\\rvec}[1]{\\boldsymbol{\\mathrm{#1}}}\n",
        "\\newcommand{\\diag}{\\mathop{\\mathrm {diag}}}\n",
        "\\newcommand{\\set}[1]{\\mathbb {#1}}\n",
        "\\newcommand{\\norm}[1]{\\left\\lVert#1\\right\\rVert}\n",
        "\\newcommand{\\pderiv}[2]{\\frac{\\partial #1}{\\partial #2}}\n",
        "\\newcommand{\\bb}[1]{\\boldsymbol{#1}}\n",
        "$$\n",
        "# Part 1: Backpropagation\n",
        "\u003ca id\u003dpart1\u003e\u003c/a\u003e"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {}
      },
      "source": [
        "In this part we will learn about **backpropagation** and **automatic differentiation**. We\u0027ll implement both of these concepts from scratch and compare our implementation to `PyTorch`\u0027s built in implementation (`autograd`)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "pycharm": {
          "is_executing": false
        }
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import unittest\n",
        "\n",
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "\n",
        "test \u003d unittest.TestCase()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {}
      },
      "source": [
        "The backpropagation algorithm is at the core of training deep models. To state the problem we\u0027ll tackle in this notebook, imagine we have an L-layer MLP model, defined as\n",
        "$$\n",
        "\\hat{\\vec{y}^i} \u003d \\vec{y}_L^i\u003d \\varphi_L \\left(\n",
        "\\mat{W}_L \\varphi_{L-1} \\left( \\cdots\n",
        "\\varphi_1 \\left( \\mat{W}_1 \\vec{x}^i + \\vec{b}_1 \\right)\n",
        "\\cdots \\right)\n",
        "+ \\vec{b}_L \\right),\n",
        "$$\n",
        "\n",
        "a pointwise loss function $\\ell(\\vec{y}, \\hat{\\vec{y}})$ and an empirical loss over our entire data set,\n",
        "$$\n",
        "L(\\vec{\\theta}) \u003d \\frac{1}{N} \\sum_{i\u003d1}^{N} \\ell(\\vec{y}^i, \\hat{\\vec{y}^i}) + R(\\vec{\\theta})\n",
        "$$\n",
        "\n",
        "where $\\vec{\\theta}$ is a vector containing all network parameters, e.g.\n",
        "$\\vec{\\theta} \u003d \\left[ \\mat{W}_{1,:}, \\vec{b}_1, \\dots,  \\mat{W}_{L,:}, \\vec{b}_L \\right]$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {}
      },
      "source": [
        "In order to train our model we would like to calculate the derivative\n",
        "(or **gradient**, in the multivariate case) of the loss with respect to each and every one of the parameters,\n",
        "i.e. $\\pderiv{L}{\\mat{W}_j}$ and $\\pderiv{L}{\\vec{b}_j}$ for all $j$.\n",
        "Since the gradient \"points\" to the direction of functional increase, the negative gradient is often used as a descent direction for descent-based optimization algorithms.\n",
        "In other words, iteratively updating each parameter proportianally to it\u0027s negetive gradient can lead to\n",
        "convergence to a local minimum of the loss function."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {}
      },
      "source": [
        "Calculus tells us that as long as we know the derivatives of all the functions \"along the way\"\n",
        "($\\varphi_i(\\cdot),\\ \\ell(\\cdot,\\cdot),\\ R(\\cdot)$)\n",
        "we can use the **chain rule** to calculate the derivative \n",
        "of the loss with respect to any one of the parameter vectors.\n",
        "Note that if the loss $L(\\vec{\\theta})$ is scalar (which is usually the case), the gradient of a parameter\n",
        "will have the same shape as the parameter itself (matrix/vector/tensor of same dimensions).\n",
        "\n",
        "For deep models that are a composition of many functions, calculating the gradient of each parameter by hand and implementing hard-coded gradient derivations quickly becomes infeasible.\n",
        "Additionally, such code makes models hard to change, since any change potentially requires re-derivation and re-implementation of the entire gradient function.\n",
        "\n",
        "The backpropagation algorithm, which we saw [in the lecture](https://vistalab-technion.github.io/cs236605/lecture_notes/lecture_3/#error-backpropagation), provides us with a effective method of applying the **chain rule** recursively so that we can implement gradient calculations of arbitrarily deep or complex models."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {}
      },
      "source": [
        "We\u0027ll now implement backpropagation using a modular software design which will allow us to chain many components layers together and get automatic gradient calculation of the output with respect to the input or any intermediate parameter."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {}
      },
      "source": [
        "To do this, we\u0027ll define a (very poorly named) class: `Block`. Here\u0027s the API of this class:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "pycharm": {
          "is_executing": false
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "text": [
            "Help on class Block in module hw2.blocks:\n\nclass Block(abc.ABC)\n |  A block is some computation element in a network architecture which\n |  supports automatic differentiation using forward and backward functions.\n |  \n |  Method resolution order:\n |      Block\n |      abc.ABC\n |      builtins.object\n |  \n |  Methods defined here:\n |  \n |  __call__(self, *args, **kwargs)\n |      Call self as a function.\n |  \n |  __init__(self)\n |      Initialize self.  See help(type(self)) for accurate signature.\n |  \n |  backward(self, dout)\n |      Computes the backward pass of the block, i.e. the gradient\n |      calculation of the final network output with respect to each of the\n |      parameters of the forward function.\n |      :param dout: The gradient of the network with respect to the\n |      output of this block.\n |      :return: A tuple with the same number of elements as the parameters of\n |      the forward function. Each element will be the gradient of the\n |      network output with respect to that parameter.\n |  \n |  forward(self, *args, **kwargs)\n |      Computes the forward pass of the block.\n |      :param args: The computation arguments (implementation specific).\n |      :return: The result of the computation.\n |  \n |  params(self)\n |      :return: Block\u0027s trainable parameters and their gradients as a list\n |      of tuples, each tuple containing a tensor and it\u0027s corresponding\n |      gradient tensor.\n |  \n |  train(self, training_mode\u003dTrue)\n |      Changes the mode of this block between training and evaluation (test)\n |      mode. Some blocks have different behaviour depending on mode.\n |      :param training_mode: True: set the model in training mode. False: set\n |      evaluation mode.\n |  \n |  ----------------------------------------------------------------------\n |  Data descriptors defined here:\n |  \n |  __dict__\n |      dictionary for instance variables (if defined)\n |  \n |  __weakref__\n |      list of weak references to the object (if defined)\n |  \n |  ----------------------------------------------------------------------\n |  Data and other attributes defined here:\n |  \n |  __abstractmethods__ \u003d frozenset({\u0027backward\u0027, \u0027forward\u0027, \u0027params\u0027})\n\n"
          ],
          "output_type": "stream"
        }
      ],
      "source": [
        "import hw2.blocks as blocks\n",
        "help(blocks.Block)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {}
      },
      "source": [
        "In other words, a `Block` can be anything: a layer, an activation function, a loss function or generally *any computation that we know how to derive a gradient for*.\n",
        "\n",
        "Each block must define a `forward()` function and a `backward()` function.\n",
        "- The `forward()` function performs the actual calculation/operation of the block and returns an output.\n",
        "- The `backward()` function computes the gradient of the input and parameters as a function of the gradient of the **output**, according to the chain rule.\n",
        "\n",
        "This may sound confusing, so here\u0027s a diagram illustrating what a `Block` does:\n",
        "\n",
        "\u003cimg src\u003d\"imgs/backprop.png\" width\u003d\"900\" /\u003e"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {}
      },
      "source": [
        "Note that the diagram doesn\u0027t show that if $f(\\vec{x},\\vec{y})$ is parametrized, there are also gradients to calculate for the parameters.\n",
        "\n",
        "The forward pass is pretty straightforward: just do the computation.\n",
        "To understand the backward pass, imagine that there\u0027s some \"downstream\" loss function\n",
        "$L(\\vec{\\theta})$ and magically somehow we are told the gradient of that loss with respect\n",
        "to the output of our block, $\\pderiv{L}{\\vec{z}}$.\n",
        "\n",
        "Now, since $f(\\vec{x},\\vec{y})$ is a function we know how to derivate,\n",
        "it means we know how to calculate $\\pderiv{\\vec{z}}{\\vec{x}}$, $\\pderiv{\\vec{z}}{\\vec{y}}$ and also the derivatives with respect to any parameters that $f$ uses.\n",
        "Thanks to the chain rule, this is all we need to calculate the gradients of the **loss** w.r.t. the input and\n",
        "parameters."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {}
      },
      "source": [
        "## Comparison with PyTorch\n",
        "\u003ca id\u003dpart1_1\u003e\u003c/a\u003e\n",
        "\n",
        "PyTorch has the `nn.Module` base class, which is similar to our `Block` since it also represents a computation element in a network.\n",
        "However PyTorch\u0027s `nn.Module`s don\u0027t compute the gradient,\n",
        "only perform the forward calculations.\n",
        "Instead, the `autograd` module tracks operations on tensors, creating a graph of operations, each with it\u0027s own `backward` function. Therefore, in PyTorch the `backward()` function is called on the tensors, not the modules.\n",
        "\n",
        "Here we\u0027ll implement a \"poor man\u0027s autograd\": We\u0027ll use PyTorch tensors,\n",
        "but we\u0027ll calculate and store the gradients in our blocks (or return them). The gradients we\u0027ll calculate are of the entire block, not individual operations on tensors.\n",
        "\n",
        "To test our  implementation, we\u0027ll use PyTorch\u0027s `autograd`.\n",
        "\n",
        "Note that of course this method of tracking gradients is **much** more limited than what PyTorch offers. However it allows us to implement the backpropagation algorithm very simply and really see how it works.\n",
        "\n",
        "Let\u0027s set up some testing instrumentation:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "pycharm": {
          "is_executing": false
        }
      },
      "outputs": [],
      "source": [
        "from hw2.grad_compare import compare_block_to_torch\n",
        "\n",
        "def test_block_grad(block: blocks.Block, x, y\u003dNone, delta\u003d1e-2):\n",
        "    diffs \u003d compare_block_to_torch(block, x, y)\n",
        "    \n",
        "    # Assert diff values\n",
        "    for diff in diffs:\n",
        "        test.assertLess(diff, delta)\n",
        "\n",
        "# Show the compare function\n",
        "compare_block_to_torch??"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {}
      },
      "source": [
        "Notes:\n",
        "- After you complete your implementation, you should make sure to read and understand the `compare_block_to_torch()` function. It will help you understand what PyTorch is doing.\n",
        "- The value of `delta` above is somewhat arbitrary. A correct implementation will give you a `diff` of zero."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {}
      },
      "source": [
        "## Block Implementations\n",
        "\u003ca id\u003dpart1_2\u003e\u003c/a\u003e\n",
        "\n",
        "We\u0027ll now implement some `Block`s that will enable us to later build an MLP model of arbitrary depth, complete with automatic differentiation.\n",
        "\n",
        "For each block, you\u0027ll first implement the `forward()` function.\n",
        "Then, you will calculate the derivative of the block by hand with respect to each of its\n",
        "input tensors and each of its parameter tensors (if any).\n",
        "Using your manually-calculated derivation, you can then implement the `backward()` function."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {}
      },
      "source": [
        "### Linear (fully connected) layer\n",
        "\n",
        "First, we\u0027ll implement an affine transform layer, also known as a fully connected layer.\n",
        "\n",
        "Given an input $\\vec{x}$ the layer computes,\n",
        "\n",
        "$$\n",
        "\\vec{z} \u003d \\vec{x} \\mattr{W}  + \\vec{b} ,~\n",
        "\\mat{W}\\in\\set{R}^{D_{\\mathrm{out}}\\times D_{\\mathrm{in}}},~ \\vec{b}\\in\\set{R}^{D_{\\mathrm{out}}}.\n",
        "$$\n",
        "\n",
        "Notes:\n",
        "- We write it this way to follow the implementation conventions.\n",
        "- In the code, the input $\\vec{x}$ will always be a tensor containing a batch dimension first. Thanks to broadcasting, $\\vec{b}$ can remain a vector even if the input is a matrix."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {}
      },
      "source": [
        "**TODO**: Complete the implementation of the `Linear` class in the `hw2/blocks.py` module."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "pycharm": {
          "is_executing": false
        }
      },
      "outputs": [],
      "source": [
        "N \u003d 100\n",
        "in_features \u003d 200\n",
        "num_classes \u003d 10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "pycharm": {
          "is_executing": false
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "text": [
            "Comparing gradients... \ninput    diff\u003d0.000\nparam#01 diff\u003d0.000\nparam#02 diff\u003d0.000\n"
          ],
          "output_type": "stream"
        }
      ],
      "source": "# Test Linear\nfc \u003d blocks.Linear(in_features, 1000)\nparams \u003d fc.params()\nx_test \u003d torch.randn(N, in_features)\n\n# Test forward pass\nz \u003d fc(x_test)\ntest.assertSequenceEqual(z.shape, [N, 1000])\n\n# Test backward pass\ntest_block_grad(fc, x_test)"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {}
      },
      "source": [
        "### Activation functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {}
      },
      "source": [
        "#### ReLU\n",
        "\n",
        "ReLU, or rectified linear unit is a very common activation function in deep learning architectures.\n",
        "In it\u0027s most standard form, as we\u0027ll implement here, it has no parameters.\n",
        "\n",
        "The ReLU operation is defined as\n",
        "\n",
        "$$\n",
        "\\mathrm{relu}(\\vec{x}) \u003d \\max(\\vec{0},\\vec{x})\n",
        "$$\n",
        "\n",
        "Note that it\u0027s not strictly differentiable, however it has sub-gradients. The gradients w.r.t. $\\vec{x}$ are simply $1$ for any positive-valued elements and zero for negative-valued elements."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {}
      },
      "source": [
        "**TODO**: Complete the implementation of the `ReLU` class in the `hw2/blocks.py` module."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "pycharm": {
          "is_executing": false
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "text": [
            "Comparing gradients... \ninput    diff\u003d0.000\n"
          ],
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": [
            "C:\\Users\\Gilad\\PycharmProjects\\DL2\\hw2\\blocks.py:158: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  return dout * torch.tensor(r \u003e 0, dtype\u003dtorch.float)\n"
          ],
          "output_type": "stream"
        }
      ],
      "source": [
        "# Test ReLU\n",
        "relu \u003d blocks.ReLU()\n",
        "x_test \u003d torch.randn(N, in_features)\n",
        "\n",
        "# Test forward pass\n",
        "z \u003d relu(x_test)\n",
        "test.assertSequenceEqual(z.shape, x_test.shape)\n",
        "\n",
        "# Test backward pass\n",
        "test_block_grad(relu, x_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {}
      },
      "source": [
        "#### Sigmoid\n",
        "\n",
        "The sigmoid function $\\sigma(x)$ is also sometimes used as an activation function.\n",
        "We have also seen it previously in the context of logistic regression.\n",
        "\n",
        "The sigmoid function is defined as\n",
        "\n",
        "$$\n",
        "\\sigma(\\vec{x}) \u003d \\frac{1}{1+\\exp(-\\vec{x})}.\n",
        "$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "pycharm": {
          "is_executing": false
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "text": [
            "Comparing gradients... \ninput    diff\u003d0.000\n"
          ],
          "output_type": "stream"
        }
      ],
      "source": [
        "# Test Sigmoid\n",
        "sigmoid \u003d blocks.Sigmoid()\n",
        "x_test \u003d torch.randn(N, in_features)\n",
        "\n",
        "# Test forward pass\n",
        "z \u003d sigmoid(x_test)\n",
        "test.assertSequenceEqual(z.shape, x_test.shape)\n",
        "\n",
        "# Test backward pass\n",
        "test_block_grad(sigmoid, x_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {}
      },
      "source": [
        "### Cross-Entropy Loss\n",
        "\n",
        "As you know by know, this is a common loss function for classification tasks.\n",
        "In class, we defined it as \n",
        "\n",
        "$$\\ell_{\\mathrm{CE}}(\\vec{y},\\hat{\\vec{y}}) \u003d - {\\vectr{y}} \\log(\\hat{\\vec{y}})$$\n",
        "\n",
        "where $\\hat{\\vec{y}} \u003d \\mathrm{softmax}(x)$ is a probability vector (the output of softmax on the class scores $\\vec{x}$) and the vector $\\vec{y}$ is a 1-hot encoded class label.\n",
        "\n",
        "However, it\u0027s tricky to compute the gradient of softmax, so instead we\u0027ll define a version of cross-entropy that produces the exact same output but works directly on the class scores $\\vec{x}$.\n",
        "\n",
        "We can write,\n",
        "$$\\begin{align}\n",
        "\\ell_{\\mathrm{CE}}(\\vec{y},\\hat{\\vec{y}}) \u0026\u003d - {\\vectr{y}} \\log(\\hat{\\vec{y}}) \n",
        "\u003d - {\\vectr{y}} \\log\\left(\\mathrm{softmax}(\\vec{x})\\right) \\\\\n",
        "\u0026\u003d - {\\vectr{y}} \\log\\left(\\frac{e^{\\vec{x}}}{\\sum_k e^{x_k}}\\right) \\\\\n",
        "\u0026\u003d - \\log\\left(\\frac{e^{x_y}}{\\sum_k e^{x_k}}\\right) \\\\\n",
        "\u0026\u003d - \\left(\\log\\left(e^{x_y}\\right) - \\log\\left(\\sum_k e^{x_k}\\right)\\right)\\\\\n",
        "\u0026\u003d - x_y + \\log\\left(\\sum_k e^{x_k}\\right)\n",
        "\\end{align}$$\n",
        "\n",
        "Where the scalar $y$ is the correct class label, so $x_y$ is the correct class score.\n",
        "\n",
        "Note that this version of cross entroy is also what\u0027s [provided](https://pytorch.org/docs/stable/nn.html#crossentropyloss) by PyTorch\u0027s `nn` module."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {}
      },
      "source": [
        "**TODO**: Complete the implementation of the `CrossEntropyLoss` class in the `hw2/blocks.py` module."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "pycharm": {
          "is_executing": false
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "text": [
            "loss\u003d 2.7283620834350586\nComparing gradients... \ninput    diff\u003d0.000\n"
          ],
          "output_type": "stream"
        }
      ],
      "source": "# Test CrossEntropy\ncross_entropy \u003d blocks.CrossEntropyLoss()\nscores \u003d torch.randn(N, num_classes)\nlabels \u003d torch.randint(low\u003d0, high\u003dnum_classes, size\u003d(N,), dtype\u003dtorch.long)\n\n# Test forward pass\nloss \u003d cross_entropy(scores, labels)\nexpected_loss \u003d torch.nn.functional.cross_entropy(scores, labels)\ntest.assertLess(torch.abs(expected_loss-loss).item(), 1e-5)\nprint(\u0027loss\u003d\u0027, loss.item())\n\n# Test backward pass\ntest_block_grad(cross_entropy, scores, y\u003dlabels)"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {}
      },
      "source": [
        "## Building Models\n",
        "\u003ca id\u003dpart1_3\u003e\u003c/a\u003e\n",
        "\n",
        "Now that we have some building `Block`s, we can build an MLP model of arbitrary depth and compute end-to-end gradients.\n",
        "\n",
        "First, lets copy an idea from PyTorch and implement our own version of the `nn.Sequential` `Module`.\n",
        "This is a `Block` which contains other `Block`s and calls them in sequence. We\u0027ll use this to build our MLP model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {}
      },
      "source": [
        "**TODO**: Complete the implementation of the `Sequential` class in the `hw2/blocks.py` module."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "pycharm": {
          "is_executing": false
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "text": [
            "Comparing gradients... \ninput    diff\u003d0.000\nparam#01 diff\u003d0.000\nparam#02 diff\u003d0.000\nparam#03 diff\u003d0.000\nparam#04 diff\u003d0.000\nparam#05 diff\u003d0.000\nparam#06 diff\u003d0.000\nparam#07 diff\u003d0.000\nparam#08 diff\u003d0.000\nparam#09 diff\u003d0.000\nparam#10 diff\u003d0.000\nparam#11 diff\u003d0.000\nparam#12 diff\u003d0.000\nparam#13 diff\u003d0.000\nparam#14 diff\u003d0.000\n"
          ],
          "output_type": "stream"
        }
      ],
      "source": "# Test Sequential\n# Let\u0027s create a long sequence of blocks and see\n# whether we can compute end-to-end gradients of the whole thing.\n# Show the compare function\ncompare_block_to_torch??\n\nseq \u003d blocks.Sequential(\n    blocks.Linear(in_features, 100),\n    blocks.Linear(100, 200),\n    blocks.Linear(200, 100),\n    blocks.ReLU(),\n    blocks.Linear(100, 500),\n    blocks.Linear(500, 200),\n    blocks.ReLU(),\n    blocks.Linear(200, 500),\n    blocks.ReLU(),\n    blocks.Linear(500, 1),\n    blocks.Sigmoid(),\n)\nx_test \u003d torch.randn(N, in_features)\n\n# Test forward pass\nz \u003d seq(x_test)\ntest.assertSequenceEqual(z.shape, [N, 1])\n\n# Test backward pass\ntest_block_grad(seq, x_test)"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {}
      },
      "source": [
        "Now, equipped with a `Sequential`, all we have to do is create an MLP architecture.\n",
        "We\u0027ll define our MLP with the following hyperparameters:\n",
        "- Number of input features, $D$.\n",
        "- Number of output classes, $C$.\n",
        "- Sizes of hidden layers, $h_1,\\dots,h_L$.\n",
        "\n",
        "So the architecture will be:\n",
        "\n",
        "FC($D$, $h_1$) $\\rightarrow$ ReLU $\\rightarrow$\n",
        "FC($h_1$, $h_2$) $\\rightarrow$ ReLU $\\rightarrow$\n",
        "$\\cdots$ $\\rightarrow$\n",
        "FC($h_{L-1}$, $h_L$) $\\rightarrow$ ReLU $\\rightarrow$\n",
        "FC($h_{L}$, $C$)\n",
        "\n",
        "We\u0027ll also create a sequence of the above MLP and a cross-entropy loss, since it\u0027s the gradient of the loss that we need in order to train a model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {}
      },
      "source": [
        "**TODO**: Complete the implementation of the `MLP` class in the `hw2/models.py` module. Ignore the `dropout` parameter for now."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "pycharm": {
          "is_executing": false
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "text": [
            "MLP, Sequential\n\t[0] Linear(200, 100)\n\t[1] ReLU\n\t[2] Linear(100, 50)\n\t[3] ReLU\n\t[4] Linear(50, 100)\n\t[5] ReLU\n\t[6] Linear(100, 10)\n\n"
          ],
          "output_type": "stream"
        }
      ],
      "source": "import hw2.models as models\n\n# Create MLP model\nmlp \u003d models.MLP(in_features, num_classes, hidden_features\u003d[100, 50, 100])\nprint(mlp)\n"
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "pycharm": {
          "is_executing": false
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "text": [
            "MLP loss\u003d2.3119688034057617, activation\u003drelu\n",
            "Comparing gradients... \ninput    diff\u003d0.000\nparam#01 diff\u003d0.010\nparam#02 diff\u003d0.026\nparam#03 diff\u003d0.026\nparam#04 diff\u003d0.035\nparam#05 diff\u003d0.061\nparam#06 diff\u003d0.080\nparam#07 diff\u003d0.104\nparam#08 diff\u003d0.131\nparam#09 diff\u003d0.010\nparam#10 diff\u003d0.026\nparam#11 diff\u003d0.026\nparam#12 diff\u003d0.035\nparam#13 diff\u003d0.061\nparam#14 diff\u003d0.080\nparam#15 diff\u003d0.104\nparam#16 diff\u003d0.131\nparam#17 diff\u003d0.010\nparam#18 diff\u003d0.026\nparam#19 diff\u003d0.026\nparam#20 diff\u003d0.035\nparam#21 diff\u003d0.061\nparam#22 diff\u003d0.080\nparam#23 diff\u003d0.104\nparam#24 diff\u003d0.131\n"
          ],
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": [
            "C:\\Users\\Gilad\\PycharmProjects\\DL2\\hw2\\blocks.py:158: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  return dout * torch.tensor(r \u003e 0, dtype\u003dtorch.float)\n"
          ],
          "output_type": "stream"
        },
        {
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[1;32m\u003cipython-input-15-3426d4b2dfa4\u003e\u001b[0m in \u001b[0;36m\u003cmodule\u003e\u001b[1;34m\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m     \u001b[1;31m# Test backward pass\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---\u003e 33\u001b[1;33m     \u001b[0mtest_block_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mseq_mlp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m\u003d\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[1;32m\u003cipython-input-4-9d5b2ff96f39\u003e\u001b[0m in \u001b[0;36mtest_block_grad\u001b[1;34m(block, x, y, delta)\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[1;31m# Assert diff values\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mdiff\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdiffs\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----\u003e 8\u001b[1;33m         \u001b[0mtest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0massertLess\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdiff\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdelta\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;31m# Show the compare function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\Anaconda3\\envs\\cs236605-hw\\lib\\unittest\\case.py\u001b[0m in \u001b[0;36massertLess\u001b[1;34m(self, a, b, msg)\u001b[0m\n\u001b[0;32m   1224\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0ma\u001b[0m \u001b[1;33m\u003c\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1225\u001b[0m             \u001b[0mstandardMsg\u001b[0m \u001b[1;33m\u003d\u001b[0m \u001b[1;34m\u0027%s not less than %s\u0027\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0msafe_repr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msafe_repr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-\u003e 1226\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfail\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_formatMessage\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstandardMsg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1227\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1228\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0massertLessEqual\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[1;33m\u003d\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\Anaconda3\\envs\\cs236605-hw\\lib\\unittest\\case.py\u001b[0m in \u001b[0;36mfail\u001b[1;34m(self, msg)\u001b[0m\n\u001b[0;32m    678\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mfail\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[1;33m\u003d\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    679\u001b[0m         \u001b[1;34m\"\"\"Fail immediately, with the given message.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--\u003e 680\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfailureException\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    681\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    682\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0massertFalse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexpr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[1;33m\u003d\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;31mAssertionError\u001b[0m: tensor(0.0263) not less than 0.01"
          ],
          "ename": "AssertionError",
          "evalue": "tensor(0.0263) not less than 0.01",
          "output_type": "error"
        }
      ],
      "source": "# Test MLP architecture\nN \u003d 100\nin_features \u003d 10\nnum_classes \u003d 10\nfor activation in (\u0027relu\u0027, \u0027sigmoid\u0027):\n    mlp \u003d models.MLP(in_features, num_classes, hidden_features\u003d[100, 50, 100], activation\u003dactivation)\n    test.assertEqual(len(mlp.sequence), 7)\n    \n    num_linear \u003d 0\n    for b1, b2 in zip(mlp.sequence, mlp.sequence[1:]):\n        if (str(b2).lower() \u003d\u003d activation):\n            test.assertTrue(str(b1).startswith(\u0027Linear\u0027))\n            num_linear +\u003d 1\n            \n    test.assertTrue(str(mlp.sequence[-1]).startswith(\u0027Linear\u0027))\n    test.assertEqual(num_linear, 3)\n\n    # Test MLP gradients\n    # Test forward pass\n    x_test \u003d torch.randn(N, in_features)\n    labels \u003d torch.randint(low\u003d0, high\u003dnum_classes, size\u003d(N,), dtype\u003dtorch.long)\n    z \u003d mlp(x_test)\n    test.assertSequenceEqual(z.shape, [N, num_classes])\n\n    # Create a sequence of MLPs and CE loss\n    # Note: deliberately using the same MLP instance multiple times to create a recurrence.\n    seq_mlp \u003d blocks.Sequential(mlp, mlp, mlp, blocks.CrossEntropyLoss())\n    loss \u003d seq_mlp(x_test, y\u003dlabels)\n    test.assertEqual(loss.dim(), 0)\n    print(f\u0027MLP loss\u003d{loss}, activation\u003d{activation}\u0027)\n\n    # Test backward pass\n    test_block_grad(seq_mlp, x_test, y\u003dlabels)"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {}
      },
      "source": [
        "If the above tests passed then congratulations - you\u0027ve now implemented an arbitrarily deep model and loss function with end-to-end automatic differentiation!"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}