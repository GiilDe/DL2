{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {}
      },
      "source": [
        "$$\n",
        "\\newcommand{\\mat}[1]{\\boldsymbol {#1}}\n",
        "\\newcommand{\\mattr}[1]{\\boldsymbol {#1}^\\top}\n",
        "\\newcommand{\\matinv}[1]{\\boldsymbol {#1}^{-1}}\n",
        "\\newcommand{\\vec}[1]{\\boldsymbol {#1}}\n",
        "\\newcommand{\\vectr}[1]{\\boldsymbol {#1}^\\top}\n",
        "\\newcommand{\\rvar}[1]{\\mathrm {#1}}\n",
        "\\newcommand{\\rvec}[1]{\\boldsymbol{\\mathrm{#1}}}\n",
        "\\newcommand{\\diag}{\\mathop{\\mathrm {diag}}}\n",
        "\\newcommand{\\set}[1]{\\mathbb {#1}}\n",
        "\\newcommand{\\norm}[1]{\\left\\lVert#1\\right\\rVert}\n",
        "\\newcommand{\\pderiv}[2]{\\frac{\\partial #1}{\\partial #2}}\n",
        "\\newcommand{\\bb}[1]{\\boldsymbol{#1}}\n",
        "$$\n",
        "# Part 3: Convolutional Architectures\n",
        "\u003ca id\u003dpart3\u003e\u003c/a\u003e"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {}
      },
      "source": [
        "In this part we will explore convolution networks and the effects of their architecture on accuracy. We\u0027ll implement a common block-based deep CNN pattern and we\u0027ll perform various experiments on it while varying the architecture. Then we\u0027ll implement our own custom architecture to see whether we can get high classification results on a large subset of CIFAR-10.\n",
        "\n",
        "Training will be performed on GPU."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {}
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import re\n",
        "import sys\n",
        "import glob\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import unittest\n",
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as tvtf\n",
        "\n",
        "%matplotlib inline\n",
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "\n",
        "seed \u003d 42\n",
        "device \u003d torch.device(\u0027cuda\u0027 if torch.cuda.is_available() else \u0027cpu\u0027)\n",
        "\n",
        "plt.rcParams.update({\u0027font.size\u0027: 12})\n",
        "test \u003d unittest.TestCase()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {}
      },
      "source": [
        "## Convolutional layers and networks\n",
        "\u003ca id\u003dpart3_1\u003e\u003c/a\u003e"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {}
      },
      "source": [
        "Convolutional layers are the most essential building blocks of the state of the art deep learning image classification models and also play an important role in many other tasks.\n",
        "As we saw in the tutorial, convolutional layers operate on and produce volumes (3D tensors) of activations.\n",
        "\n",
        "\n",
        "One way to think about them is as if the neurons are organized in a 3D grid,\n",
        "where neurons at the same depth share weights (represented here as colors).\n",
        "Contrary to fully connected (affine) layers, neurons in convolutional layers are **not** connected to each of the activations of the previous layer.\n",
        "Instead, each neuron is connected only to a small region of the input volume e.g. a 5x5x$C_{\\mathrm{in}}$ slice\n",
        "(where $C_{\\mathrm{in}}$ is the input volume\u0027s depth).\n",
        "\n",
        "\u003cimg src\u003d\"imgs/depthcol.jpeg\" /\u003e\n",
        "\n",
        "Another way to interpret convolutional layers is as a collection of 3D learnable filters,\n",
        "each of which operates on a small spatial region of the input volume.\n",
        "Each filter is convolved with the input volume (\"slides over it\"),\n",
        "and a dot product is computed at each location followed by a non-linearity which produces one activation.\n",
        "All these activations produce a 2D plane known as a **feature map**.\n",
        "Multiple feature maps (one for each filter) comprise the output volume.\n",
        "\n",
        "\u003cimg src\u003d\"imgs/cnn_filters.png\" width\u003d\"600\" /\u003e\n",
        "\n",
        "A crucial property of convolutional layers is their translation invariance,\n",
        "i.e. their ability to detect features regardelss of their spatial location in the input.\n",
        "\n",
        "Convolutional network architectures usually follow a pattern basic repeating blocks: one or more convolution layers, each followed by a non-linearity (generally ReLU) and then a pooling layer to reduce spatial dimensions. Usually, the number of convolutional filters increases the deeper they are in the network.\n",
        "These layers are meant to extract features from the input.\n",
        "Then, one or more fully-connected layers is used to combine the extracted features into the required number of output class scores."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {}
      },
      "source": [
        "## Building convolutional networks with PyTorch\n",
        "\u003ca id\u003dpart3_2\u003e\u003c/a\u003e"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {}
      },
      "source": [
        "PyTorch provides all the basic building blocks needed for creating a convolutional arcitecture within the [`torch.nn`](https://pytorch.org/docs/stable/nn.html) package.\n",
        "Let\u0027s use them to create a basic convolutional network with the following architecture pattern:\n",
        "\n",
        "    [(CONV -\u003e ReLU)*P -\u003e MaxPool]*(N/P) -\u003e (Linear -\u003e ReLU)*M -\u003e Linear\n",
        "\n",
        "Here $N$ is the total number of convolutional layers,\n",
        "$P$ specifies how many convolutions to perform before each pooling layer\n",
        "and $M$ specifies the number of hidden fully-connected layers before the final output layer."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {}
      },
      "source": [
        "**TODO**: Complete the implementaion of the `ConvClassifier` class in the `hw2/models.py` module."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {}
      },
      "outputs": [],
      "source": [
        "import hw2.models as models\n",
        "torch.manual_seed(seed)\n",
        "\n",
        "net \u003d models.ConvClassifier((3,100,100), 10, filters\u003d[32]*4, pool_every\u003d2, hidden_dims\u003d[100]*2).to(device)\n",
        "print(net)\n",
        "\n",
        "test_image \u003d torch.randint(low\u003d0, high\u003d256, size\u003d(3, 100, 100), dtype\u003dtorch.float).to(device)\n",
        "test_out \u003d net(test_image.unsqueeze(0))\n",
        "print(\u0027out \u003d\u0027, test_out)\n",
        "\n",
        "expected_out \u003d torch.load(\u0027tests/assets/expected_conv_out.pt\u0027).to(device)\n",
        "test.assertLess(torch.norm(test_out - expected_out).item(), 1e-5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {}
      },
      "source": [
        "**Note about running on GPUs**.\n",
        "\n",
        "Notice how we called `.to(device)` on **both** the model and the input tensor.\n",
        "Here the `device` is a `torch.device` object that we created above. If an nvidia GPU is available on the machine you\u0027re running this on, the `device` will be `\u0027cuda\u0027`. When you run `.to(device)` on a model, it recursively goes over all the model parameter tensors and copies their memory to the GPU. Similarly, calling `.to(device)` on the input image also copies it.\n",
        "\n",
        "In order to train on a GPU, you need to make sure to move **all** your tensors to it. You\u0027ll get errors if you try to mix CPU and GPU tensors in a computation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {}
      },
      "outputs": [],
      "source": [
        "print(f\u0027This notebook is running with device\u003d{device}\u0027)\n",
        "print(f\u0027The model parameter tensors are therefore also on device\u003d{next(net.parameters()).device}\u0027)\n",
        "print(f\u0027The test image is therefore also on device\u003d{test_image.device}\u0027)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {}
      },
      "source": [
        "Let\u0027s load CIFAR-10 again to use as our dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {}
      },
      "outputs": [],
      "source": [
        "data_dir \u003d os.path.expanduser(\u0027~/.pytorch-datasets\u0027)\n",
        "ds_train \u003d torchvision.datasets.CIFAR10(root\u003ddata_dir, download\u003dTrue, train\u003dTrue, transform\u003dtvtf.ToTensor())\n",
        "ds_test \u003d torchvision.datasets.CIFAR10(root\u003ddata_dir, download\u003dTrue, train\u003dFalse, transform\u003dtvtf.ToTensor())\n",
        "\n",
        "print(f\u0027Train: {len(ds_train)} samples\u0027)\n",
        "print(f\u0027Test: {len(ds_test)} samples\u0027)\n",
        "\n",
        "x0,_ \u003d ds_train[0]\n",
        "in_size \u003d x0.shape\n",
        "num_classes \u003d 10\n",
        "print(\u0027input image size \u003d\u0027, in_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {}
      },
      "source": [
        "Now as usual, as a sanity test let\u0027s make sure we can overfit a tiny dataset with our model. But first we need to adapt our `Trainer` for PyTorch models."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {}
      },
      "source": [
        "**TODO**: Complete the implementaion of the `TorchTrainer` class in the `hw2/training.py` module."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {}
      },
      "outputs": [],
      "source": "import hw2.training as training\ntorch.manual_seed(seed)\n\n# Define a tiny part of the CIFAR-10 dataset to overfit it\nbatch_size \u003d 2\nmax_batches \u003d 25\ndl_train \u003d torch.utils.data.DataLoader(ds_train, batch_size, shuffle\u003dFalse)\n\n# Create model, loss and optimizer instances\nmodel \u003d models.ConvClassifier(in_size, num_classes, filters\u003d[32], pool_every\u003d1, hidden_dims\u003d[100])\nloss_fn \u003d torch.nn.CrossEntropyLoss()\noptimizer \u003d torch.optim.SGD(model.parameters(), lr\u003d1e-2, momentum\u003d0.9,)\n\n# Use TorchTrainer to run only the training loop a few times.\ntrainer \u003d training.TorchTrainer(model, loss_fn, optimizer, device)\nbest_acc \u003d 0\nfor i in range(22):\n    res \u003d trainer.train_epoch(dl_train, max_batches\u003dmax_batches, verbose\u003d(i%2\u003d\u003d0))\n    best_acc \u003d res.accuracy if res.accuracy \u003e best_acc else best_acc\n    \n# Test overfitting\ntest.assertGreaterEqual(best_acc, 95)"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {}
      },
      "source": [
        "## Experimenting with model architectures\n",
        "\u003ca id\u003dpart3_3\u003e\u003c/a\u003e"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {}
      },
      "source": [
        "You will now perform a series of experiments that train various model configurations on a much larger part of the CIFAR-10 dataset.\n",
        "\n",
        "To perform the experiments, you\u0027ll need to use a machine with a GPU since training time might be too long otherwise.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {}
      },
      "source": [
        "### Notes on using course servers\n",
        "\n",
        "First, please read the [course servers guide](https://vistalab-technion.github.io/cs236605/assignments/hpc-servers/) carefully.\n",
        "\n",
        "To run the experiments on the course servers, you can use the `py-sbatch.sh` script directly to perform a single experiment run in batch mode (since it runs python once), or use the `srun` command to do a single run in interactive mode. For example, running a single run of experiment 1 interactively (after `conda activate` of course):\n",
        "\n",
        "```shell\n",
        "srun -c 2 --gres\u003dgpu:1 --pty python -m hw2.experiments run-exp -n test -K 32 64 -L 2 -P 2 -H 100\n",
        "```\n",
        "\n",
        "To perform multiple runs in batch mode with `sbatch` (e.g. for running all the configurations of an experiments), you can create your own script based on `py-sbatch.sh` and invoke whatever commands you need within it.\n",
        "\n",
        "Please don\u0027t request more than **2 CPU** cores and **1 GPU** device for your runs. The code won\u0027t be able to utilize more than that anyway, so you\u0027ll see no performance gain if you do. It will only cause delays for other students using the servers."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {}
      },
      "source": [
        "### General notes for running experiments\n",
        "\n",
        "- You can run the experiments on a different machine (e.g. the course servers) and copy the results (files)\n",
        "to the `results` folder on your local machine.\n",
        "This notebook will only display the results, not run the actual experiment code (except for a demo run).\n",
        "\n",
        "- It\u0027s important to give each experiment run a name as specified by the notebook instructions later on. The each run has a `run_name` parameter that will also be the name of the results file which this notebook will expect to load.\n",
        "\n",
        "- You will implement the code to run the experiments in the `hw2/experiments.py` module. This module has a CLI parser so that you can invoke it as a script and pass in all the configuration parameters for a single experiment run.\n",
        "\n",
        "- You should use `python -m hw2.experiments run-exp` to run an experiment, and **not** `python hw2/experiments.py run-exp`, regardless of how/where you run it."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {}
      },
      "source": [
        "### Experiment 1 - Network depth and number of filters"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {}
      },
      "source": [
        "In this part we will test some different architecture configurations based on our `ConvClassifier`.\n",
        "Specifically, we want to try different depths and number of features to see the effects these parameters have on the model\u0027s performance.\n",
        "\n",
        "To do this, we\u0027ll define two extra hyperparameters for our model, `K` (`filters_per_layer`) and `L` (`layers_per_block`).\n",
        "- `K` is a list, containing the number of filters we want to have in our conv layers.\n",
        "- `L` is the number of consecutive layers with the same number of filters to use.\n",
        "\n",
        "For example, if `K\u003d[32, 64]` and `L\u003d2` it means we want two conv layers with 32 filters followed by two conv layers with 64 filters. The feature-extraction part of our model will therefore be:\n",
        "\n",
        "    Conv(X,32)-\u003eReLu-\u003eConv(32,32)-\u003eReLU-\u003eMaxPool-\u003eConv(32,64)-\u003eReLU-\u003eConv(64,64)-\u003eReLU-\u003eMaxPool\n",
        "    \n",
        "We\u0027ll try various values of the `K` and `L` parameters in combination and see how each architecture trains. **All other hyperparameters are up to you**, including the choice of the optimization algorithm, the learning rate, regularization and architecture hyperparams such as `pad_every` and `hidden_dims`. You can try some manual runs to determine some good values for the hyperparameters or implement cross-validation to do it.\n",
        "However, the **dataset size** you test on should be large. Use at least ~12000 training images and ~3000 validation images. The **batch size** parameter however is also up to you.\n",
        "\n",
        "The important thing is that you state what you used, how you decided on it, and explain your results based on that."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {}
      },
      "source": [
        "First we need to write some code to run the experiment.\n",
        "\n",
        "**TODO**:\n",
        "1. Implement the `run_experiment()` function in the `hw2/experiments.py` module.\n",
        "1. If you haven\u0027t done so already, it would be an excellent idea to implement the **early stopping** feature of the `Trainer` class.\n",
        "\n",
        "The following block tests that your implementation works. It\u0027s also meant to show you that each experiment run creates a result file containing the parameters to reproduce and the `FitResult` object for plotting."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {}
      },
      "outputs": [],
      "source": [
        "import hw2.experiments as experiments\n",
        "from hw2.experiments import load_experiment\n",
        "from cs236605.plot import plot_fit\n",
        "\n",
        "# Test experiment1 implementation on a few data samples and with a small model\n",
        "experiments.run_experiment(\u0027test_run\u0027, seed\u003dseed, bs_train\u003d50, batches\u003d10, epochs\u003d10, early_stopping\u003d5,\n",
        "                           filters_per_layer\u003d[32], layers_per_block\u003d1, pool_every\u003d1, hidden_dims\u003d[100])\n",
        "\n",
        "# There should now be a file \u0027test_run.json\u0027 in your `results/` folder.\n",
        "# We can use it to load the results of the experiment.\n",
        "cfg, fit_res \u003d load_experiment(\u0027results/test_run.json\u0027)\n",
        "_, _ \u003d plot_fit(fit_res)\n",
        "\n",
        "# And `cfg` contains the exact parameters to reproduce it\n",
        "print(\u0027experiment config: \u0027, cfg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {}
      },
      "source": [
        "We\u0027ll use the following function to load multiple experiment results and plot them together."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {}
      },
      "outputs": [],
      "source": [
        "def plot_exp_results(filename_pattern, results_dir\u003d\u0027results\u0027):\n",
        "    fig \u003d None\n",
        "    result_files \u003d glob.glob(os.path.join(results_dir, filename_pattern))\n",
        "    result_files.sort()\n",
        "    if len(result_files) \u003d\u003d 0:\n",
        "        print(f\u0027No results found for pattern {filename_pattern}.\u0027, file\u003dsys.stderr)\n",
        "        return\n",
        "    for filepath in result_files:\n",
        "        m \u003d re.match(\u0027exp\\d_(\\d_)?(.*)\\.json\u0027, os.path.basename(filepath))\n",
        "        cfg, fit_res \u003d load_experiment(filepath)\n",
        "        fig, axes \u003d plot_fit(fit_res, fig, legend\u003dm[2],log_loss\u003dTrue)\n",
        "    del cfg[\u0027filters_per_layer\u0027]\n",
        "    del cfg[\u0027layers_per_block\u0027]\n",
        "    print(\u0027common config: \u0027, cfg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {}
      },
      "source": [
        "#### Experiment 1.1: Varying the network depth (`L`)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {}
      },
      "source": [
        "First, we\u0027ll test the effect of the network depth on training.\n",
        "\n",
        "**Configuratons**:\n",
        "- `K\u003d32` fixed, with `L\u003d2,4,8,16` varying per run\n",
        "- `K\u003d64` fixed, with `L\u003d2,4,8,16` varying per run\n",
        "\n",
        "So 8 different runs in total.\n",
        "\n",
        "**Naming runs**:\n",
        "Each run should be named `exp1_1_K{}_L{}` where the braces are placeholders for the values. For example, the first run should be named `exp1_1_K32_L2`. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {}
      },
      "source": [
        "**TODO**: Run the experiment on the above configuration. Make sure the result file names are as expected. Use the following blocks to display the results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {}
      },
      "outputs": [],
      "source": [
        "plot_exp_results(\u0027exp1_1_K32*.json\u0027)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {}
      },
      "outputs": [],
      "source": [
        "plot_exp_results(\u0027exp1_1_K64*.json\u0027)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {}
      },
      "source": [
        "#### Experiment 1.2: Varying the number of filters per layer (`K`)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {}
      },
      "source": [
        "Now we\u0027ll test the effect of the number of convolutional filters in each layer.\n",
        "\n",
        "**Configuratons**:\n",
        "- `L\u003d2` fixed, with `K\u003d[32],[64],[128],[258]` varying per run.\n",
        "- `L\u003d4` fixed, with `K\u003d[32],[64],[128],[258]` varying per run.\n",
        "- `L\u003d8` fixed, with `K\u003d[32],[64],[128],[258]` varying per run.\n",
        "\n",
        "So 12 different runs in total. To clarify, each run `K` takes the value of a list with a single element.\n",
        "\n",
        "**Naming runs**:\n",
        "Each run should be named `exp1_2_L{}_K{}` where the braces are placeholders for the values. For example, the first run should be named `exp1_2_L2_K32`. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {}
      },
      "source": [
        "**TODO**: Run the experiment on the above configuration. Make sure the result file names are as expected. Use the following blocks to display the results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {}
      },
      "outputs": [],
      "source": [
        "plot_exp_results(\u0027exp1_2_L2*.json\u0027)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {}
      },
      "outputs": [],
      "source": [
        "plot_exp_results(\u0027exp1_2_L4*.json\u0027)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {}
      },
      "outputs": [],
      "source": [
        "plot_exp_results(\u0027exp1_2_L8*.json\u0027)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {}
      },
      "source": [
        "#### Experiment 1.3: Varying both the number of filters (`K`) and network depth (`L`)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {}
      },
      "source": [
        "Now we\u0027ll test the effect of the number of convolutional filters in each layer.\n",
        "\n",
        "**Configuratons**:\n",
        "- `K\u003d[64, 128, 256]` fixed with `L\u003d1,2,3,4` varying per run.\n",
        "\n",
        "So 4 different runs in total. To clarify, each run `K` takes the value of an array with a three elements.\n",
        "\n",
        "**Naming runs**:\n",
        "Each run should be named `exp1_3_L{}_K{}-{}-{}` where the braces are placeholders for the values. For example, the first run should be named `exp1_3_L1_K64-128-256`. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {}
      },
      "source": [
        "**TODO**: Run the experiment on the above configuration. Make sure the result file names are as expected. Use the following blocks to display the results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {}
      },
      "outputs": [],
      "source": [
        "plot_exp_results(\u0027exp1_3*.json\u0027)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {}
      },
      "source": [
        "### Experiment 2 - Custom network architecture"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {}
      },
      "source": [
        "In this part you will create your own custom network architecture based on the `ConvClassifier` you\u0027ve implemented.\n",
        "\n",
        "Try to overcome some of the limitations your experiment 1 results, using what you learned in the course.\n",
        "\n",
        "You are free to add whatever you like to the model, for instance \n",
        "- Batch normalization\n",
        "- Dropout layers\n",
        "- Skip connections\n",
        "- Change kernel spatial sizes and strides\n",
        "- Custom blocks or ideas from known architectures (e.g. inception module)\n",
        "\n",
        "Just make sure to keep the model\u0027s `init` API identical (or maybe just add parameters)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {}
      },
      "source": [
        "**TODO**: Implement your custom architecture in the `YourCodeNet` class within the `hw2/models.py` module."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {}
      },
      "outputs": [],
      "source": [
        "net \u003d models.YourCodeNet((3,100,100), 10, filters\u003d[32]*4, pool_every\u003d2, hidden_dims\u003d[100]*2).to(device)\n",
        "print(net)\n",
        "\n",
        "test_image \u003d torch.randint(low\u003d0, high\u003d256, size\u003d(3, 100, 100), dtype\u003dtorch.float).to(device)\n",
        "test_out \u003d net(test_image.unsqueeze(0))\n",
        "print(\u0027out \u003d\u0027, test_out)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {}
      },
      "source": [
        "#### Experiment 2 Configuration"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {}
      },
      "source": [
        "Run your custom model on at least the following:\n",
        "\n",
        "**Configuratons**:\n",
        "- `K\u003d[64, 128, 256, 512]` fixed with `L\u003d1,2,3,4` varying per run.\n",
        "\n",
        "So 4 different runs in total. To clarify, each run `K` takes the value of an array with a four elements.\n",
        "\n",
        "If you want, you can add some extra following the same pattern.\n",
        "Try to see how deep a model you can train.\n",
        "\n",
        "**Naming runs**:\n",
        "Each run should be named `exp2_L{}_K{}-{}-{}-{}` where the braces are placeholders for the values. For example, the first run should be named `exp2_L1_K64-128-256-512`. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {}
      },
      "source": [
        "**TODO**: Run the experiment on the above configuration. Make sure the result file names are as expected. Use the following blocks to display the results. To make the experiment runner use your custom network model instead of the previous one, use the `--ycn` argument on the command line."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {}
      },
      "outputs": [],
      "source": [
        "plot_exp_results(\u0027exp2*.json\u0027)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {}
      },
      "source": [
        "## Questions\n",
        "\u003ca id\u003dpart3_4\u003e\u003c/a\u003e"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {}
      },
      "source": [
        "**TODO** Answer the following questions. Write your answers in the appropriate variables in the module `hw2/answers.py`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {}
      },
      "outputs": [],
      "source": [
        "from cs236605.answers import display_answer\n",
        "import hw2.answers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {}
      },
      "source": [
        "### Question 1 \n",
        "\n",
        "Analyze your results from experiment 1.1. In particular,\n",
        "1.  Explain the effect of depth on the accuracy. What depth produces the best results and why do you think that\u0027s the case?\n",
        "1. Were there values of `L` for which the network wasn\u0027t trainable? what causes this? Suggest two things which may be done to resolve it at least partially."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {}
      },
      "outputs": [],
      "source": [
        "display_answer(hw2.answers.part3_q1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {}
      },
      "source": [
        "### Question 2 \n",
        "\n",
        "Analyze your results from experiment 1.2. In particular, compare to the results of experiment 1.1."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {}
      },
      "outputs": [],
      "source": [
        "display_answer(hw2.answers.part3_q2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {}
      },
      "source": [
        "### Question 3 \n",
        "\n",
        "Analyze your results from experiment 1.3."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {}
      },
      "outputs": [],
      "source": [
        "display_answer(hw2.answers.part3_q3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {}
      },
      "source": [
        "### Question 4 \n",
        "\n",
        "1. Explain your modifications to the architecture which you implemented in the `YourCodeNet` class.\n",
        "2. Analyze the results of experiment 2. Compare to experiment 1."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {}
      },
      "outputs": [],
      "source": [
        "display_answer(hw2.answers.part3_q4)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}